{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN6TkUOSacpIfTliU+DWiUV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Initialize"],"metadata":{"id":"VKWVyhQuAeSQ"}},{"cell_type":"markdown","source":["## Check if we are using CPU or GPU"],"metadata":{"id":"A9EgXia0AqPN"}},{"cell_type":"code","source":["import tensorflow as tf\n","tf.test.gpu_device_name()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2bj6sieRIChN","executionInfo":{"status":"ok","timestamp":1743611000083,"user_tz":-480,"elapsed":12825,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"fcf6780b-4e35-414a-c499-3ffafeed42cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["## Import the requirments utils or libs"],"metadata":{"id":"2EDzStxRBXfp"}},{"cell_type":"code","source":["!pip install tensorflow==2.18.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a8zyJtTTLlX","executionInfo":{"status":"ok","timestamp":1743610971953,"user_tz":-480,"elapsed":46884,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"b70374c3-89a1-4b82-ef33-072cf12c8f89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.18.0\n","  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.13.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.71.0)\n","Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)\n","  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.13.0)\n","Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow==2.18.0)\n","  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n","Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml_dtypes 0.5.1\n","    Uninstalling ml_dtypes-0.5.1:\n","      Successfully uninstalled ml_dtypes-0.5.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.19.0\n","    Uninstalling tensorboard-2.19.0:\n","      Successfully uninstalled tensorboard-2.19.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.19.0\n","    Uninstalling tensorflow-2.19.0:\n","      Successfully uninstalled tensorflow-2.19.0\n","Successfully installed ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTvolsVDBpBD"},"outputs":[],"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","source":["# Beginner Version Model (Easy, Simple)"],"metadata":{"id":"LbC-Yc2_A0Sa"}},{"cell_type":"markdown","source":["## Preparing the dataset\n","* We use MNIST as the dataset"],"metadata":{"id":"_m-CI-0-IZ_3"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","\n","# Load dataset, note that x_train, y_train, x_test, y_test are numpy arrays\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Convert numpy arrays to tensors\n","x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) # [60000, 28, 28]\n","y_train = tf.convert_to_tensor(y_train, dtype=tf.int32) # [60000]\n","x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) # [10000, 28, 28]\n","y_test = tf.convert_to_tensor(y_test, dtype=tf.int32) # [10000]\n","\n","# Scale the dataset and add a channel dimension\n","x_train = x_train / 255.0\n","x_train = tf.expand_dims(x_train, axis=-1) # [60000, 28, 28, 1]\n","x_test = x_test / 255.0\n","x_test = tf.expand_dims(x_test, axis=-1) # [60000, 28, 28, 1]\n","\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIymRvwnIg9d","executionInfo":{"status":"ok","timestamp":1743611019840,"user_tz":-480,"elapsed":2577,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"d892263a-a380-4155-8f5f-0aa7a48eba61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","(60000, 28, 28, 1)\n","(60000,)\n","(10000, 28, 28, 1)\n","(10000,)\n"]}]},{"cell_type":"markdown","source":["## Build the beginner version model"],"metadata":{"id":"5sLuBZYS8epo"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n","\n","beginner_model = Sequential([\n","    Conv2D(filters=6, kernel_size=8, activation='relu'),\n","    MaxPooling2D(pool_size=(2, 2)),\n","    Conv2D(filters=15, kernel_size=4, activation='relu'),\n","    MaxPooling2D(pool_size=(2, 2)),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dense(10, activation='softmax') # Outputs a probability distribution\n","])"],"metadata":{"id":"jtRuzimY7GCw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training & Evaluating the beginner version model"],"metadata":{"id":"Lxna_WAU9-Kd"}},{"cell_type":"code","source":["beginner_model.compile(\n","  optimizer='adam',\n","  loss='sparse_categorical_crossentropy',\n","  metrics=['accuracy'] # Percentage of good predictions\n",")\n","\n","beginner_model.fit(x_train, y_train, epochs=3, batch_size=1024)\n","\n","beginner_model.evaluate(x_test, y_test, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DuzAM33o8y-Y","executionInfo":{"status":"ok","timestamp":1743611030686,"user_tz":-480,"elapsed":10838,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"df08e2a5-ea89-4721-e815-30b38e1bebde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.5078 - loss: 1.7675\n","Epoch 2/3\n","\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8857 - loss: 0.3849\n","Epoch 3/3\n","\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9240 - loss: 0.2591\n","313/313 - 2s - 5ms/step - accuracy: 0.9384 - loss: 0.2019\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.2019142061471939, 0.9383999705314636]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Expert Version Model (Hard, Complex)"],"metadata":{"id":"Mm1UsLzrAbfc"}},{"cell_type":"markdown","source":["## Preparing the dataset"],"metadata":{"id":"n-vrcyQsftT2"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","\n","# Load dataset, note that x_train, y_train, x_test, y_test are numpy arrays\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Convert numpy arrays to tensors\n","x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) # [60000, 28, 28]\n","y_train = tf.convert_to_tensor(y_train, dtype=tf.int32) # [60000]\n","x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) # [10000, 28, 28]\n","y_test = tf.convert_to_tensor(y_test, dtype=tf.int32) # [10000]\n","\n","# Scale the dataset and add a channel dimension\n","x_train = x_train / 255.0\n","x_train = tf.expand_dims(x_train, axis=-1) # [60000, 28, 28, 1]\n","x_test = x_test / 255.0\n","x_test = tf.expand_dims(x_test, axis=-1) # [60000, 28, 28, 1]\n","\n","# Make sure the shapes are correct\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","\n","# Split dataset with batch size of 1024\n","BATCH_SIZE = 1024\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n","print(train_dataset)\n","print(test_dataset)"],"metadata":{"id":"xlarABrr-GS-","executionInfo":{"status":"ok","timestamp":1743612688622,"user_tz":-480,"elapsed":878,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"03860974-6cf9-4fbd-bab1-8905db2e3d2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28, 1)\n","(60000,)\n","(10000, 28, 28, 1)\n","(10000,)\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"]}]},{"cell_type":"markdown","source":["## Build custom layers for export version model\n","* Structure:\n","  - Inputs Layer\n","  - Hidden Layer 1: ConvPool2D\n","  - Hidden Layer 2: ConvPool2D\n","  - Hidden Layer 3: Flatten\n","  - Hidden Layer 4: DoubleDense\n","  - Outputs Layer\n","* The above structure is a basic CNN model for classify graphs or pictures\n","* **Note**: `__init__` is called when creating the layer, `build` is called when the first time the layer is used, and `call` is what the layer actually does."],"metadata":{"id":"hkOlUGl7gVk0"}},{"cell_type":"markdown","source":["### Initialize algorithms for layers\n","* 如同上述結構，我們會需要以下算法來幫助我們建立各層"],"metadata":{"id":"RtX1mcp90AM2"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"],"metadata":{"id":"kCP2DoyI0AEB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Linear\n","* 全連接層, Fully Connected Layer\n","* 這層通常是 神經網路中最基本的層\n","* 主要是在做線性變換，透過此公式：$y = Wx + b$\n","  - $W$: weighted matrix\n","  - $b$: bias\n","* 用於 將輸入轉換成不同的特徵空間，例如從 128 維度轉換成 64 維度。\n","* 因為這邊是要用於做圖像分類，所以 $W$ 為權重矩陣。\n","* 應用：最後的分類層（softmax 通常跟 Linear 一起用），將特徵數壓縮或展開"],"metadata":{"id":"P3A1Ho4Slh2q"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Layer\n","\n","class Linear(Layer):\n","  \"\"\"y = Wx + b\"\"\"\n","\n","  def __init__(self, units=32):\n","    super(Linear, self).__init__()\n","    self.units = units\n","\n","  def build(self, input_shape):\n","    self.W = self.add_weight(\n","      shape=(input_shape[-1], self.units),\n","      initializer='random_normal',\n","      trainable=True\n","    )\n","    self.b = self.add_weight(\n","      shape=(self.units, ),\n","      initializer='random_normal',\n","      trainable=True\n","    )\n","\n","  def call(self, inputs):\n","    # inputs -> W*inputs + b -> return\n","    return tf.matmul(inputs, self.W) + self.b"],"metadata":{"id":"3iehcsEAfsnc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Double Dense\n","* 將兩個 Dense（全連接層）疊加起來：linear_1, linear2\n","* 搭配兩個 activation function（激活函數）：ReLU, Softmax\n","  - ReLU（Rectified Linear Unit）:\n","    - 這是一個非線性激活函數：$f(x) = max(0, x)$\n","    - 用於讓網路可以學習**非線性特徵**（避免只有線性轉換）\n","    - 相比於 sigmoid（另一個 activation function），ReLU不會有梯度消失的問題（ vanishing gradient ），因此訓練速度快、效果更好\n","\n","  - Softmax:\n","    - 這是一個 歸一化函數：$\\sigma(z)i = \\frac{e^{z_i}}{\\sum{j=1}^{n} e^{z_j}}$\n","    - 通常用在**最後一層輸出，讓輸出變成「機率分佈」**，適合用來做分類問題\n","    - 可以把原始數值轉換成機率\n","    - 適合多分類問題（multi-class classficication）"],"metadata":{"id":"BsXt-i4Plm4b"}},{"cell_type":"code","source":["class DoubleDense(Layer):\n","  \"\"\"Linear-relu + Linear-softmax\"\"\"\n","\n","  def __init__(self, nb_classes):\n","    super(DoubleDense, self).__init__()\n","    self.nb_classes = nb_classes\n","\n","  def build(self, input_shape):\n","    self.linear_1 = Linear(units=128)\n","    self.linear_2 = Linear(units=self.nb_classes)\n","\n","  def call(self, inputs):\n","    # inputs -> Linear(128) + ReLU -> Linear(10) + Softmax -> return\n","    x = tf.nn.relu(self.linear_1(inputs))\n","    x = tf.nn.softmax(self.linear_2(x))\n","    return x"],"metadata":{"id":"QeUXuZZqpJqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ConvPool2D\n","* 此隱藏層其實包含了兩個部分：Conv（卷積層）+ ReLU（激活函數），MaxPooling2D（池化層）\n","* Conv2D（卷積層）：\n","  - 用 kernel（卷積核）來掃描圖片，提取特徵（邊緣、形狀、輪廓、紋理等）。\n","  - 產生的結果將會是另一張新的圖片，但像素值會是經過處理的\n","\n","* ReLU（激活函數）：\n","  - 類似之前在 Linear 中的 ReLU，此處的是通常在 Conv2D 層中都會加入的，用於把任何小於0的數字變成0\n","  - 如此一來可以**去除雜音**，讓神經網路學到更有意義的特徵\n","\n","* MaxPooling2D（最大池化層）:\n","  - **用於降低維度（ downsampling ），以減少計算量**\n","  - 概念：**讓最重要的特徵保留下來**，去除不重要的細節\n","  - 用一個**小窗口（通常是2x2）**，在圖片上滑動（ ex. stride=2 ），**每個區塊只保留最大值**"],"metadata":{"id":"AOEWC-Jdynxo"}},{"cell_type":"code","source":["class ConvPool2D(Layer):\n","  \"\"\"Conv2D-relu + MaxPooling2D\"\"\"\n","\n","  def __init__(self, nb_kernels, kernel_size):\n","    super(ConvPool2D, self).__init__()\n","    self.nb_kernels = nb_kernels\n","    self.kernel_size = kernel_size\n","\n","  def build(self, input_shape):\n","    self.conv_2D = Conv2D(\n","      filters=self.nb_kernels,\n","      kernel_size=self.kernel_size,\n","      activation='relu'\n","    )\n","    self.pool_2D = MaxPooling2D(pool_size=(2, 2))\n","\n","  def call(self, inputs):\n","    # inputs -> Convolution -> Max Pooling -> return\n","    x = self.conv_2D(inputs)\n","    x = self.pool_2D(x)\n","    return x"],"metadata":{"id":"s4edzRT-yrr9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define the model using the above layers"],"metadata":{"id":"H5y0nl7_BpNx"}},{"cell_type":"code","source":["from tensorflow.keras import Model\n","\n","class ExportModel(Model):\n","  def __init__(self, nb_classes):\n","    super(ExportModel, self).__init__()\n","    self.nb_classes = nb_classes\n","\n","    # Make sure to initialize these variables,\n","    # or we will get wrong while we invoke train_steps() in the training loop\n","    self.x_0 = None\n","    self.x_1 = None\n","    self.x_2 = None\n","    self.predictions = None\n","\n","  def build(self, input_shape):\n","    self.conv_pool_1 = ConvPool2D(nb_kernels=6, kernel_size=8)\n","    self.conv_pool_2 = ConvPool2D(nb_kernels=15, kernel_size=4)\n","    self.flatten = Flatten()\n","    self.double_dense = DoubleDense(nb_classes=self.nb_classes)\n","\n","  def call(self, inputs):\n","    self.x_0 = self.conv_pool_1(inputs)\n","    self.x_1 = self.conv_pool_2(self.x_0)\n","    self.x_2 = self.flatten(self.x_1)\n","    self.predictions = self.double_dense(self.x_2)\n","    return self.predictions"],"metadata":{"id":"tLr9uobvCKZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating the model using pre-made functions"],"metadata":{"id":"Lm8S_gFNCzIl"}},{"cell_type":"code","source":["loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","export_model = ExportModel(nb_classes=10)\n","export_model.compile(optimizer=optimizer, loss=loss_function)"],"metadata":{"id":"aN-goV_RC2YV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"id":"FJtqex4kEJwa"}},{"cell_type":"code","source":["export_model.fit(train_dataset, epochs=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2eGkaXODVLy","executionInfo":{"status":"ok","timestamp":1743611036278,"user_tz":-480,"elapsed":4850,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"d29ec13a-75a4-4784-84db-54f3e0e00b05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 1.9921\n","Epoch 2/3\n","\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4520\n","Epoch 3/3\n","\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2987\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7c2c2011cf90>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### The Summary of the model"],"metadata":{"id":"jseuNP0lEZIN"}},{"cell_type":"code","source":["export_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"aQGDNfbBEcft","executionInfo":{"status":"ok","timestamp":1743611036301,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"0087fa1d-3f77-471a-ba33-2957eee6d019"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"export_model\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"export_model\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv_pool2d (\u001b[38;5;33mConvPool2D\u001b[0m)             │ ?                           │             \u001b[38;5;34m390\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv_pool2d_1 (\u001b[38;5;33mConvPool2D\u001b[0m)           │ ?                           │           \u001b[38;5;34m1,455\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m135\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ double_dense (\u001b[38;5;33mDoubleDense\u001b[0m)           │ ?                           │          \u001b[38;5;34m18,698\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv_pool2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvPool2D</span>)             │ ?                           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv_pool2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvPool2D</span>)           │ ?                           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,455</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ double_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DoubleDense</span>)           │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,698</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,631\u001b[0m (240.75 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,631</span> (240.75 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,543\u001b[0m (80.25 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,543</span> (80.25 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m41,088\u001b[0m (160.50 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> (160.50 KB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Build a custom training loop"],"metadata":{"id":"o6Whm-pVHyXV"}},{"cell_type":"markdown","source":["### Initialize the loss function and optimizer we will use in the training loop\n","* `loss_function` 是用來在之後的 training loop 的每次執行時會計算 batch 的 loss，例如：`loss = loss_function(y_true, y_pred)`，這樣會得到單一批次（ batch ）的 loss\n","* `optimizer` 也是在 training loop 的每次執行時去依照計算的 loss 來調整模型\n","* `train_loss` 和 `train_accuracy` 則是用來累計並追蹤 loss 和 accuracy\n","  - `ts.keras.metrics.Mean(name='train_loss')` 和 `tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')` 這兩者是 Tensorflow 的 metric 物件，專門用來累計並計算整個 epoch 的 loss 和 accuracy，而不是只計算單次 batch\n","* `test_loss` 和 `test_accuracy` 則是用來在測試模型（我們會用不同的資料來對模型做訓練和測試）時，累計並追蹤 loss 和 accuracy"],"metadata":{"id":"AXSQg2m5IV1D"}},{"cell_type":"code","source":["loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n","\n","optimizer = tf.keras.optimizers.Adam()\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","# !Important: We have to reset the above variables every time before we do the training loop"],"metadata":{"id":"M93F6nTpHxaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building the training step"],"metadata":{"id":"lmQYXoivL0bt"}},{"cell_type":"code","source":["def train_step(images, labels, model, loss_function, optimizer):\n","\n","  # Open a GradientTape\n","  with tf.GradientTape() as tape:\n","    # Forward pass\n","    predictions = model(images)\n","\n","    # Calculate the loss for this batch\n","    loss = loss_function(labels, predictions)\n","\n","  # Get gradient of loss (weights)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","\n","  # Update weights\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  # Calculate the loss and accuracy\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)"],"metadata":{"id":"PEMU8T2WL9-Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building a testing step"],"metadata":{"id":"HFPXB_NSP5FR"}},{"cell_type":"code","source":["def test_step(images, labels, model, loss_function, optimizer):\n","  # Forward pass\n","  predictions = model(images)\n","\n","  # Calculate the loss for this batch\n","  loss = loss_function(labels, predictions)\n","\n","  # Note that we don't do optimize here,\n","  # since we are only testing the model\n","\n","  # Save loss and accuracy\n","  test_loss(loss)\n","  test_accuracy(labels, predictions)"],"metadata":{"id":"A_W5Nec-P0Ex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Build the custom training loop"],"metadata":{"id":"pKlSvEAgQgBk"}},{"cell_type":"code","source":["# Since the train_loss, train_accuracy are traced by Tensorflow keras,\n","# we have to reset it here\n","loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","export_model = ExportModel(nb_classes=10)\n","epochs = 3\n","start = time.time()\n","\n","# Iterate over epochs\n","for epoch in range(epochs):\n","\n","  # Train over every batch in the training dataset\n","  for images, labels in train_dataset:\n","    train_step(images, labels, export_model, loss_function, optimizer)\n","\n","  # Test over every batch in the testing dataset\n","  for test_images, test_labels in test_dataset:\n","    test_step(test_images, test_labels, export_model, loss_function, optimizer)\n","\n","  # Print the results for this epoch\n","  template = 'Epoch {:.0f}, Loss: {:.3f}, Accuracy: {:.3f}%, Test Loss: {:.3f}, Test Accuracy: {:.3f}%'\n","  print(template.format(epoch + 1,\n","                        train_loss.result(),\n","                        train_accuracy.result() * 100,\n","                        test_loss.result(),\n","                        test_accuracy.result() * 100))\n","\n","  # Reset the metrics for the next epoch\n","  train_loss.reset_state()\n","  train_accuracy.reset_state()\n","  test_loss.reset_state()\n","  test_accuracy.reset_state()\n","\n","# Display elapsed time\n","end = time.time()\n","print(\"Time: \", end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InZjY8ZRQfF2","executionInfo":{"status":"ok","timestamp":1743612932542,"user_tz":-480,"elapsed":11857,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"9e804ec3-2a04-4e24-d022-5f7ca39509c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.422, Accuracy: 65.697, Test Loss: 0.466, Test Accuracy: 86.180\n","Epoch 2, Loss: 0.408, Accuracy: 88.222, Test Loss: 0.306, Test Accuracy: 91.020\n","Epoch 3, Loss: 0.293, Accuracy: 91.355, Test Loss: 0.234, Test Accuracy: 93.040\n","Time:  11.845203638076782\n"]}]},{"cell_type":"markdown","source":["### Building a **graph** to speed up training\n","* By adding @tf.function decorator before the training and testing functions"],"metadata":{"id":"XwdRWgUvYrwC"}},{"cell_type":"code","source":["@tf.function\n","def graph_train_step(images, labels, model, loss_function, optimizer):\n","\n","  # Open a GradientTape\n","  with tf.GradientTape() as tape:\n","    # Forward pass\n","    predictions = model(images)\n","\n","    # Calculate the loss for this batch\n","    loss = loss_function(labels, predictions)\n","\n","  # Get gradient of loss (weights)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","\n","  # Update weights\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  # Save loss and accuracy\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)"],"metadata":{"id":"KDCMAaUDYpk4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def graph_test_step(images, labels, model, loss_function, optimizer):\n","  # Forward pass\n","  predictions = model(images)\n","\n","  # Calculate the loss for this batch\n","  loss = loss_function(labels, predictions)\n","\n","  # Note that we don't do optimize here,\n","  # since we are only testing the model\n","\n","  # Save loss and accuracy\n","  test_loss(loss)\n","  test_accuracy(labels, predictions)"],"metadata":{"id":"5UfzcAzVY-vc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Notice the time different between the original training loop**"],"metadata":{"id":"adMsFq5_ZGt1"}},{"cell_type":"code","source":["# Since the train_loss, train_accuracy are traced by Tensorflow keras,\n","# we have to reset it here\n","loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","export_model = ExportModel(nb_classes=10)\n","epochs = 3\n","start = time.time()\n","\n","# Iterate over epochs\n","for epoch in range(epochs):\n","\n","  # Train over every batch in the training dataset\n","  for images, labels in train_dataset:\n","    graph_train_step(images, labels, export_model, loss_function, optimizer)\n","\n","  # Test over every batch in the testing dataset\n","  for test_images, test_labels in test_dataset:\n","    graph_test_step(test_images, test_labels, export_model, loss_function, optimizer)\n","\n","  # Print the results for this epoch\n","  template = 'Epoch {:.0f}, Loss: {:.3f}, Accuracy: {:.3f}%, Test Loss: {:.3f}, Test Accuracy: {:.3f}%'\n","  print(template.format(epoch + 1,\n","                        train_loss.result(),\n","                        train_accuracy.result() * 100,\n","                        test_loss.result(),\n","                        test_accuracy.result() * 100))\n","\n","  # Reset the metrics for the next epoch\n","  train_loss.reset_state()\n","  train_accuracy.reset_state()\n","  test_loss.reset_state()\n","  test_accuracy.reset_state()\n","\n","# Display elapsed time\n","end = time.time()\n","print(\"Time: \", end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdvPFM4rZD6b","executionInfo":{"status":"ok","timestamp":1743612949359,"user_tz":-480,"elapsed":3593,"user":{"displayName":"Jeff HiIam","userId":"16929437531474341275"}},"outputId":"476660db-8121-4781-f09c-bf64345c921a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.363, Accuracy: 64.433, Test Loss: 0.561, Test Accuracy: 82.970\n","Epoch 2, Loss: 0.480, Accuracy: 85.347, Test Loss: 0.364, Test Accuracy: 89.110\n","Epoch 3, Loss: 0.346, Accuracy: 89.595, Test Loss: 0.278, Test Accuracy: 91.440\n","Time:  3.5787980556488037\n"]}]},{"cell_type":"markdown","source":["## Additions"],"metadata":{"id":"fuLNt-01b6m5"}},{"cell_type":"markdown","source":["### Adding Regularization to the loss function"],"metadata":{"id":"vHaGrmk-ex6j"}},{"cell_type":"code","source":["class ConvPool2DWithRegularization(Layer):\n","  \"\"\"Conv2D-relu + MaxPooling2D with Regularization\"\"\"\n","\n","  def __init__(self, nb_kernels, kernel_size):\n","    super(ConvPool2D, self).__init__()\n","    self.nb_kernels = nb_kernels\n","    self.kernel_size = kernel_size\n","\n","  def build(self, input_shape):\n","    self.conv_2D = Conv2D(\n","      filters=self.nb_kernels,\n","      kernel_size=self.kernel_size,\n","      activation='relu',\n","      # using keras's parameters\n","      # kernel_regularizer=tf.keras.regularizers.l2(1.)\n","    )\n","    self.pool_2D = MaxPooling2D(pool_size=(2, 2))\n","\n","  def call(self, inputs):\n","    x = self.conv_2D(inputs)\n","\n","    # using custom layer's loss property\n","    # self.l1_reg = tf.reduce_sum(tf.abs(self.W) + tf.reduce_sum(tf.abs(self.b)))\n","    # self.add_loss(self.l1_reg)\n","\n","    x = self.pool_2D(x)\n","    return x"],"metadata":{"id":"nVF_IaAHbnaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_step(images, labels, model, loss_function, optimizer):\n","\n","  with tf.GradientTape() as tape:\n","    predictions = model(images)\n","\n","    loss = loss_function(labels, predictions)\n","\n","    # Add extra losses created during this forward pass\n","    loss += 1e-3 * sum(model.losses)\n","\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)"],"metadata":{"id":"3IytyQIPdodo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Custom loss function"],"metadata":{"id":"Uyp5vmgxe1li"}},{"cell_type":"code","source":["from tensorflow.keras.losses import Loss\n","\n","class CustomLoss(Loss):\n","  \"\"\"Custom Sparse Cross Entropy loss with L1 Regularization\"\"\"\n","\n","  def __init__(self, tuning_param, model) -> None:\n","    super(CustomLoss, self).__init__()\n","    self.tuning_param = tuning_param\n","    self.SCE = tf.kerase.losses.SparseCategoricalCrossentropy()\n","    self.model = model\n","\n","  def call(self, y_true, y_pred):\n","    return self.SCE(y_true, y_pred) + self.tuning_param * sum(self.model.losses)\n","\n","# And just to replace the loss function in the training loop with `custom_loss_function` which is instantiated from `CustomLoss`"],"metadata":{"id":"AyE9Lz-BeK-c"},"execution_count":null,"outputs":[]}]}